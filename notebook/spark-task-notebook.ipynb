{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4de325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad837d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Task\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbc374",
   "metadata": {},
   "source": [
    "### Part 1: Spark RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e1f9d",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('groceries.csv', 'r') as csvfile:\n",
    "    csvtext = csvfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = []\n",
    "for i in csvtext:\n",
    "    line = i.replace(\"\\n\", \"\")\n",
    "    add = line.split(',')\n",
    "    csv.extend(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1924de",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fae6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.parallelize(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1aa38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1eca3a",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2201a",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddUnique = rdd.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b938467",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddUnique.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f73c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rddUnique.coalesce(1).saveAsTextFile(\"test4_out/out_1_2a.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c6ead",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ba87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddUnique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddUniqueCount = rddUnique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddUnique=spark.sparkContext.parallelize([rddUniqueCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddUnique.saveAsTextFile(\"test_out/out_1_2b.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebbd2c1",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea60513",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = rdd.countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddValues=spark.sparkContext.parallelize(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a141be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddValues.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bffeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddValues.saveAsTextFile(\"test_out/out_1_3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4ed92",
   "metadata": {},
   "source": [
    "### Part 2: Spark Dataframe API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f5569",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7918a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"files/part-00000.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fcb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce51395",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41deb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfe18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dadcfa5",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([f.max(\"price\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([f.min(\"price\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44df83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats = df.select([f.min(\"price\"), f.max(\"price\"), f.count(\"price\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02081860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats.coalesce(1).write.csv(\"test3_out/out_2_2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e954a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3936fedc",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c941f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"bedrooms\", \"beds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(df.price > 5000).filter(df.review_scores_value == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624449de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_filtered.select(f.mean('bathrooms'), f.mean('bedrooms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_selected.withColumnRenamed(\"avg(bathrooms)\", \"avg_bathrooms\").withColumnRenamed(\"avg(bedrooms)\", \"avg_bedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2155f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04beb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.write.csv(\"test_out/ut_2_3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6008c6",
   "metadata": {},
   "source": [
    "#### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947db6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "minPrice = df.select(\"price\").rdd.min()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople = df.filter(df.price == minPrice).select(\"review_scores_value\", 'beds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20932c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxRating = df.select(\"review_scores_value\").rdd.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople = dfPeople.filter(dfPeople.review_scores_value == maxRating).select(\"review_scores_value\", 'beds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba1980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b7d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a770bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople = df.filter(df.price == minPrice).select(\"review_scores_value\", 'beds').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPeople = df.filter(df.price == minPrice).sort(\"review_scores_value\")#.select('beds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e7494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a653e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6dd65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b10c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.price == minPrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3eb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bd568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4893f47",
   "metadata": {},
   "source": [
    "#### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9aa2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "\n",
    "with DAG(\n",
    "    \"etl_sales_daily\",\n",
    "    start_date=days_ago(1),\n",
    "    schedule_interval=None,\n",
    ") as dag:\n",
    "    \n",
    "    task1 = DummyOperator(task_id=\"task1\")\n",
    "    task2 = DummyOperator(task_id=\"task2\")\n",
    "    task3 = DummyOperator(task_id=\"task3\")\n",
    "    task4 = DummyOperator(task_id=\"task4\")\n",
    "    task5 = DummyOperator(task_id=\"task5\")\n",
    "    task6 = DummyOperator(task_id=\"task6\")\n",
    "    \n",
    "    task1 >> [task2, task3]\n",
    "    [task2, task3] >> [task4, task5, task6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c1b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bab48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce019194",
   "metadata": {},
   "source": [
    "### Part 3: Applied Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326c2f0",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a84c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1858bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1117b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Task\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44824891",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432f173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a30e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIris = spark.read.csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f975422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----------+\n",
      "|_c0|_c1|_c2|_c3|        _c4|\n",
      "+---+---+---+---+-----------+\n",
      "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
      "|4.7|3.2|1.3|0.2|Iris-setosa|\n",
      "|4.6|3.1|1.5|0.2|Iris-setosa|\n",
      "|5.0|3.6|1.4|0.2|Iris-setosa|\n",
      "|5.4|3.9|1.7|0.4|Iris-setosa|\n",
      "|4.6|3.4|1.4|0.3|Iris-setosa|\n",
      "|5.0|3.4|1.5|0.2|Iris-setosa|\n",
      "|4.4|2.9|1.4|0.2|Iris-setosa|\n",
      "|4.9|3.1|1.5|0.1|Iris-setosa|\n",
      "|5.4|3.7|1.5|0.2|Iris-setosa|\n",
      "|4.8|3.4|1.6|0.2|Iris-setosa|\n",
      "|4.8|3.0|1.4|0.1|Iris-setosa|\n",
      "|4.3|3.0|1.1|0.1|Iris-setosa|\n",
      "|5.8|4.0|1.2|0.2|Iris-setosa|\n",
      "|5.7|4.4|1.5|0.4|Iris-setosa|\n",
      "|5.4|3.9|1.3|0.4|Iris-setosa|\n",
      "|5.1|3.5|1.4|0.3|Iris-setosa|\n",
      "|5.7|3.8|1.7|0.3|Iris-setosa|\n",
      "|5.1|3.8|1.5|0.3|Iris-setosa|\n",
      "+---+---+---+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfIris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209226dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|            _c4|\n",
      "+---------------+\n",
      "| Iris-virginica|\n",
      "|    Iris-setosa|\n",
      "|Iris-versicolor|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfIris.select('_c4').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a7760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Iris-virginica' : \"1\", \"Iris-setosa\" : \"2\", \"Iris-versicolor\" : \"3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e6cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIrisLabeled = dfIris.withColumnRenamed('_c4', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73ec580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIrisLabeled = dfIrisLabeled.replace(to_replace=mapping, subset=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca24c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIrisLabeled = dfIrisLabeled.withColumn(\"label\", dfIrisLabeled.label.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff11c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7f1d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIrisInt = (dfIrisLabeled.withColumn(\"_c0\", dfIrisLabeled._c0.cast('float'))\n",
    "                 .withColumn(\"_c1\", dfIrisLabeled._c1.cast('float'))\n",
    "                 .withColumn(\"_c2\", dfIrisLabeled._c2.cast('float'))\n",
    "                 .withColumn(\"_c3\", dfIrisLabeled._c3.cast('float')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b4030d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+\n",
      "|_c0|_c1|_c2|_c3|label|\n",
      "+---+---+---+---+-----+\n",
      "|5.1|3.5|1.4|0.2|    2|\n",
      "|4.9|3.0|1.4|0.2|    2|\n",
      "|4.7|3.2|1.3|0.2|    2|\n",
      "|4.6|3.1|1.5|0.2|    2|\n",
      "|5.0|3.6|1.4|0.2|    2|\n",
      "|5.4|3.9|1.7|0.4|    2|\n",
      "|4.6|3.4|1.4|0.3|    2|\n",
      "|5.0|3.4|1.5|0.2|    2|\n",
      "|4.4|2.9|1.4|0.2|    2|\n",
      "|4.9|3.1|1.5|0.1|    2|\n",
      "|5.4|3.7|1.5|0.2|    2|\n",
      "|4.8|3.4|1.6|0.2|    2|\n",
      "|4.8|3.0|1.4|0.1|    2|\n",
      "|4.3|3.0|1.1|0.1|    2|\n",
      "|5.8|4.0|1.2|0.2|    2|\n",
      "|5.7|4.4|1.5|0.4|    2|\n",
      "|5.4|3.9|1.3|0.4|    2|\n",
      "|5.1|3.5|1.4|0.3|    2|\n",
      "|5.7|3.8|1.7|0.3|    2|\n",
      "|5.1|3.8|1.5|0.3|    2|\n",
      "+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfIrisInt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0763cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3141c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['_c0', '_c1', '_c2', '_c3'], outputCol='features')\n",
    "output = assembler.transform(dfIrisInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22784bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalised_data = output.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68468ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[5.09999990463256...|    2|\n",
      "|[4.90000009536743...|    2|\n",
      "|[4.69999980926513...|    2|\n",
      "|[4.59999990463256...|    2|\n",
      "|[5.0,3.5999999046...|    2|\n",
      "|[5.40000009536743...|    2|\n",
      "|[4.59999990463256...|    2|\n",
      "|[5.0,3.4000000953...|    2|\n",
      "|[4.40000009536743...|    2|\n",
      "|[4.90000009536743...|    2|\n",
      "|[5.40000009536743...|    2|\n",
      "|[4.80000019073486...|    2|\n",
      "|[4.80000019073486...|    2|\n",
      "|[4.30000019073486...|    2|\n",
      "|[5.80000019073486...|    2|\n",
      "|[5.69999980926513...|    2|\n",
      "|[5.40000009536743...|    2|\n",
      "|[5.09999990463256...|    2|\n",
      "|[5.69999980926513...|    2|\n",
      "|[5.09999990463256...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalised_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a023054",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = finalised_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a22e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True, labelCol='label', featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45b47f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06c11181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[4.30000019073486...|  2.0|[-2.7665473569707...|[4.83533237531611...|       2.0|\n",
      "|[4.40000009536743...|  2.0|[-2.8575961317871...|[6.02016740563160...|       2.0|\n",
      "|[4.40000009536743...|  2.0|[-2.8052348372137...|[2.54823043397787...|       2.0|\n",
      "|[4.40000009536743...|  2.0|[-2.7165255879470...|[8.33176041133090...|       2.0|\n",
      "|[4.5,2.2999999523...|  2.0|[-3.1383912021474...|[3.65114949494463...|       2.0|\n",
      "|[4.59999990463256...|  2.0|[-2.7895770595776...|[6.13743723739274...|       2.0|\n",
      "|[4.59999990463256...|  2.0|[-2.6648387811164...|[1.61453429499633...|       2.0|\n",
      "|[4.59999990463256...|  2.0|[-2.5277703847562...|[8.33864900506284...|       2.0|\n",
      "|[4.69999980926513...|  2.0|[-2.7355506106862...|[2.92155865107416...|       2.0|\n",
      "|[4.69999980926513...|  2.0|[-2.7595707791306...|[7.20255565948400...|       2.0|\n",
      "|[4.80000019073486...|  2.0|[-2.8222759173419...|[9.64613011517905...|       2.0|\n",
      "|[4.80000019073486...|  2.0|[-2.8549406583822...|[3.48415319574408...|       2.0|\n",
      "|[4.80000019073486...|  2.0|[-2.8102671611247...|[1.91326708785813...|       2.0|\n",
      "|[4.80000019073486...|  2.0|[-2.7012233932495...|[8.82061202524410...|       2.0|\n",
      "|[4.90000009536743...|  3.0|[-3.3938642557132...|[4.64769031777405...|       3.0|\n",
      "|[4.90000009536743...|  1.0|[-3.5599169341093...|[8.31690021624139...|       1.0|\n",
      "|[4.90000009536743...|  2.0|[-2.8449499615001...|[2.78421894688376...|       2.0|\n",
      "|[4.90000009536743...|  2.0|[-2.7922697426446...|[1.13196467721404...|       2.0|\n",
      "|[4.90000009536743...|  2.0|[-2.7922697426446...|[1.13196467721404...|       2.0|\n",
      "|[4.90000009536743...|  2.0|[-2.7922697426446...|[1.13196467721404...|       2.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrn_summary = fit_model.summary\n",
    "lrn_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8716c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             label|        prediction|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               109|               109|\n",
      "|   mean| 1.963302752293578| 1.963302752293578|\n",
      "| stddev|0.8042319260174194|0.8042319260174194|\n",
      "|    min|               1.0|               1.0|\n",
      "|    max|               3.0|               3.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrn_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac3ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64941ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['_c0', '_c1', '_c2', '_c3'], outputCol='features')\n",
    "output = assembler.transform(dfIrisInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c805d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a2d8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = spark.createDataFrame(\n",
    "    [(5.1, 3.5, 1.4, 0.2),\n",
    "     (6.2, 3.4, 5.4, 2.3)],\n",
    "    [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47b041d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|\n",
      "+------------+-----------+------------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|\n",
      "|         6.2|        3.4|         5.4|        2.3|\n",
      "+------------+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e81d5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol='features')\n",
    "pred_data_acc = assembler.transform(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fed765c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data_acc_ = pred_data_acc.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f0bc369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-2.6358602471710...|[3.93083743216410...|       2.0|\n",
      "|[6.2,3.4,5.4,2.3]|[-3.4132219437368...|[1.16112013199064...|       1.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_model.transform(pred_data_acc_).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c77569c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictModel(spark, model):\n",
    "    \"\"\"Predict given results model\n",
    "    \n",
    "    Args:\n",
    "        spark: pyspark.sql.session.SparkSession\n",
    "        model: LinearRegression\n",
    "    Return:\n",
    "        df: dataframe with predictions, pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    predData = spark.createDataFrame(\n",
    "        [(5.1, 3.5, 1.4, 0.2),\n",
    "         (6.2, 3.4, 5.4, 2.3)],\n",
    "        [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
    "    assembler = VectorAssembler(inputCols = ['sepal_length',\n",
    "                                             'sepal_width',\n",
    "                                             'petal_length',\n",
    "                                             'petal_width'],\n",
    "                                outputCol='features')\n",
    "    predDataAcc = assembler.transform(predData)\n",
    "    predFeatures = predDataAcc.select('features')\n",
    "    predicts = model.transform(predFeatures)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15951611",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff = predictModel(spark, fit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d2ff62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-2.6358602471710...|[3.93083743216410...|       2.0|\n",
      "|[6.2,3.4,5.4,2.3]|[-3.4132219437368...|[1.16112013199064...|       1.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "172e6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "484720fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------+\n",
      "|         features|       rawPrediction|         probability|prediction|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|[-2.6358602471710...|[3.93083743216410...|       2.0|\n",
      "|[6.2,3.4,5.4,2.3]|[-3.4132219437368...|[1.16112013199064...|       1.0|\n",
      "+-----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dffff_ = (dfff.withColumn(\"features\", f.col('features').cast(StringType()))\n",
    "          .withColumn(\"rawPrediction\", f.col('rawPrediction').cast(StringType()))\n",
    "          .withColumn(\"probability\", f.col('probability').cast(StringType()))\n",
    "          .withColumn(\"prediction\", f.col('prediction').cast(StringType()))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca564089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3cb7c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdfff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mttt.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1223\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1224\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1239\u001b[0m )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support struct<type:tinyint,size:int,indices:array<int>,values:array<double>> data type."
     ]
    }
   ],
   "source": [
    "dfff.write.csv('ttt.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b75866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdd0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adf5002f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o341.predict.\n: java.lang.ClassCastException: class org.apache.spark.sql.Dataset cannot be cast to class org.apache.spark.ml.linalg.Vector (org.apache.spark.sql.Dataset and org.apache.spark.ml.linalg.Vector are in unnamed module of loader 'app')\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predict(LogisticRegression.scala:1055)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_data_acc_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:467\u001b[0m, in \u001b[0;36mJavaPredictionModel.predict\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Predict label for the given features.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:72\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o341.predict.\n: java.lang.ClassCastException: class org.apache.spark.sql.Dataset cannot be cast to class org.apache.spark.ml.linalg.Vector (org.apache.spark.sql.Dataset and org.apache.spark.ml.linalg.Vector are in unnamed module of loader 'app')\n\tat org.apache.spark.ml.classification.LogisticRegressionModel.predict(LogisticRegression.scala:1055)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "fit_model.predict(pred_data_acc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd740ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels.predictions.show()\n",
    "eval = BinaryClassificationEvaluator(rawPredictionCol = \"prediction\", labelCol = \"churn\")\n",
    "auc = eval.evaluate(pred_labels.predictions)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c11d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437c683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
