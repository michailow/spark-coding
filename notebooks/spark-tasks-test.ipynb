{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7ac929-4428-4c95-8ca5-d45ca2cab198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task1_2a\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        return: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractCSV(filepath):\n",
    "    \"\"\"Opens CSV and return list of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        csv: list of products\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        csvtext = csvfile.readlines()\n",
    "    csv = []\n",
    "    for i in csvtext:\n",
    "        line = i.replace(\"\\n\", \"\")\n",
    "        add = line.split(',')\n",
    "        csv.extend(add)\n",
    "    return csv\n",
    "\n",
    "\n",
    "def transformRDD(spark, csv):\n",
    "    \"\"\"Transforms cvs list into Spark RDD,\n",
    "    collects unique\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        rddUnique: pyspark.rdd.RDD\n",
    "    \"\"\"\n",
    "    rdd=spark.sparkContext.parallelize(csv)\n",
    "    rddUnique = rdd.coalesce(1).distinct()\n",
    "    return rddUnique\n",
    "\n",
    "    \n",
    "def loadCSV(rdd, outputPath):\n",
    "    \"\"\"Load CSV into folder\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    rdd.saveAsTextFile(outputPath)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/groceries.csv'\n",
    "    outPutPath = '../out/out_1_2a.txt'\n",
    "    spark = startSpark()\n",
    "    csv = extractCSV(filePath)\n",
    "    rdd = transformRDD(spark, csv)\n",
    "    loadCSV(rdd, outPutPath)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98220984-e8c9-4cc2-80d5-4553bb68099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task1_2b\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        spark: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractCSV(filepath):\n",
    "    \"\"\"Opens CSV and return list of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        csv: list of products\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        csvtext = csvfile.readlines()\n",
    "    csv = []\n",
    "    for i in csvtext:\n",
    "        line = i.replace(\"\\n\", \"\")\n",
    "        add = line.split(',')\n",
    "        csv.extend(add)\n",
    "    return csv\n",
    "\n",
    "\n",
    "def transformRDD(spark, csv):\n",
    "    \"\"\"Transforms cvs list into Spark RDD,\n",
    "    collects unique and get number of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        rddUnique: pyspark.rdd.RDD\n",
    "    \"\"\"\n",
    "    rdd=spark.sparkContext.parallelize(csv)\n",
    "    rddUnique = rdd.distinct()\n",
    "    rddUniqueCount = rddUnique.count()\n",
    "    rddUnique=spark.sparkContext.parallelize([rddUniqueCount])\n",
    "    return rddUnique\n",
    "\n",
    "    \n",
    "def loadCSV(rdd, outputPath):\n",
    "    \"\"\"Load CSV into folder\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    rdd.coalesce(1).saveAsTextFile(outputPath)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/groceries.csv'\n",
    "    outPutPath = '../out/out_1_2b.txt'\n",
    "    spark = startSpark()\n",
    "    csv = extractCSV(filePath)\n",
    "    rdd = transformRDD(spark, csv)\n",
    "    loadCSV(rdd, outPutPath)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbcc665-db84-4c95-a209-9b1038b537f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task1_3\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        spark: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractCSV(filepath):\n",
    "    \"\"\"Opens CSV and return list of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        csv: list of products\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        csvtext = csvfile.readlines()\n",
    "    csv = []\n",
    "    for i in csvtext:\n",
    "        line = i.replace(\"\\n\", \"\")\n",
    "        add = line.split(',')\n",
    "        csv.extend(add)\n",
    "    return csv\n",
    "\n",
    "\n",
    "def transformRDD(spark, csv):\n",
    "    \"\"\"Transforms cvs list into Spark RDD,\n",
    "    collects unique and get number of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        return: pyspark.rdd.RDD\n",
    "    \"\"\"\n",
    "    rdd=spark.sparkContext.parallelize(csv)\n",
    "    values = rdd.countByValue().items()\n",
    "    rddValues=spark.sparkContext.parallelize(values)\n",
    "    return rddValues\n",
    "\n",
    "    \n",
    "def loadCSV(rdd, outputPath):\n",
    "    \"\"\"Load CSV into folder\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        return: None\n",
    "    \"\"\"\n",
    "    rdd.coalesce(1).saveAsTextFile(outputPath)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/groceries.csv'\n",
    "    outPutPath = '../out/out_1_3.txt'\n",
    "    spark = startSpark()\n",
    "    csv = extractCSV(filePath)\n",
    "    rdd = transformRDD(spark, csv)\n",
    "    loadCSV(rdd, outPutPath)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf4d033-0e57-433f-ba46-757302d62cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task2_2\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        spark: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractCSV(filepath):\n",
    "    \"\"\"Opens CSV and return list of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        csv: list of products\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        csvtext = csvfile.readlines()\n",
    "    csv = []\n",
    "    for i in csvtext:\n",
    "        line = i.replace(\"\\n\", \"\")\n",
    "        add = line.split(',')\n",
    "        csv.extend(add)\n",
    "    return csv\n",
    "\n",
    "\n",
    "def transformRDD(spark, csv):\n",
    "    \"\"\"Transforms cvs list into Spark RDD,\n",
    "    collects unique and get number of products\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        return: pyspark.rdd.RDD\n",
    "    \"\"\"\n",
    "    rdd=spark.sparkContext.parallelize(csv)\n",
    "    values = rdd.countByValue().items()\n",
    "    rddValues=spark.sparkContext.parallelize(values)\n",
    "    return rddValues\n",
    "\n",
    "    \n",
    "def loadCSV(rdd, outputPath):\n",
    "    \"\"\"Load CSV into folder\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to downloaded csv file\n",
    "    Return:\n",
    "        return: None\n",
    "    \"\"\"\n",
    "    rdd.coalesce(1).saveAsTextFile(outputPath)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/groceries.csv'\n",
    "    outPutPath = '../out/out_2_2.txt'\n",
    "    spark = startSpark()\n",
    "    csv = extractCSV(filePath)\n",
    "    rdd = transformRDD(spark, csv)\n",
    "    loadCSV(rdd, outPutPath)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4af969e-83db-4a62-8ae1-49cf42f88f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task2_3\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        return: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractDF(spark, filepath):\n",
    "    \"\"\"Opens parquet file\n",
    "    \n",
    "    Args:\n",
    "        spark: pyspark.sql.session.SparkSession\n",
    "        filepath: path to downloaded parquet file\n",
    "    Return:\n",
    "        df: list of products, pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(filepath)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transformDF(df):\n",
    "    \"\"\"Transforms df into stats\n",
    "    \n",
    "    Args:\n",
    "        df: raw dataframe; pyspark.sql.dataframe.DataFrame\n",
    "    Return:\n",
    "        df: prepared dataframe; pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    dfFiltered = df.filter(df.price > 5000).filter(df.review_scores_value == 10)\n",
    "    dfSelected = dfFiltered.select(f.mean('bathrooms'), f.mean('bedrooms'))\n",
    "    dfSelected = dfSelected.withColumnRenamed(\"avg(bathrooms)\",\n",
    "                                                \"avg_bathrooms\").withColumnRenamed(\"avg(bedrooms)\",\n",
    "                                                                                   \"avg_bedrooms\")\n",
    "    \n",
    "    return dfSelected\n",
    "\n",
    "    \n",
    "def loadDF(df, outputPath):\n",
    "    \"\"\"Load DF into folder\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark.sql.dataframe.DataFrame\n",
    "        outputPath: path to save files\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.coalesce(1).write.csv(outputPath)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/part-00000.parquet'\n",
    "    outPutPath = '../out/out_2_3.txt'\n",
    "    \n",
    "    spark = startSpark()\n",
    "    df = extractDF(spark, filePath)\n",
    "    dfPrep = transformDF(df)\n",
    "    loadDF(dfPrep, outPutPath)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5001a46-ea7d-46cb-b0ab-5f9141ec5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task2_4\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        return: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractDF(spark, filepath):\n",
    "    \"\"\"Opens parquet file\n",
    "    \n",
    "    Args:\n",
    "        spark: pyspark.sql.session.SparkSession\n",
    "        filepath: path to downloaded parquet file\n",
    "    Return:\n",
    "        df: list of products, pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(filepath)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transformDF(df):\n",
    "    \"\"\"Transforms df into stats\n",
    "    \n",
    "    Args:\n",
    "        df: raw dataframe; pyspark.sql.dataframe.DataFrame\n",
    "    Return:\n",
    "        df: prepared dataframe; pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    minPrice = df.select(\"price\").rdd.min()[0]\n",
    "    dfPeople = df.filter(df.price == minPrice).select(\"review_scores_value\", 'beds')\n",
    "    maxRating = df.select(\"review_scores_value\").rdd.max()[0]\n",
    "    dfPeople = dfPeople.filter(dfPeople.review_scores_value == maxRating).select(\"review_scores_value\", 'beds')\n",
    "    \n",
    "    return dfPeople\n",
    "\n",
    "    \n",
    "def loadDF(df, outputPath):\n",
    "    \"\"\"Load DF into folder\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark.sql.dataframe.DataFrame\n",
    "        outputPath: path to save files\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.coalesce(1).write.csv(outputPath)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/part-00000.parquet'\n",
    "    outPutPath = '../out/out_2_4.txt'\n",
    "    \n",
    "    spark = startSpark()\n",
    "    df = extractDF(spark, filePath)\n",
    "    dfPrep = transformDF(df)\n",
    "    loadDF(dfPrep, outPutPath)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "797debbe-deea-4267-b8c8-aba2a40c2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task3_2\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "def startSpark(name=\"spark-etl\"):\n",
    "    \"\"\"Start Spark Sessions\n",
    "    \n",
    "    Args:\n",
    "        param name: Spark job name\n",
    "    Return:\n",
    "        return: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(name)\n",
    "             .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "\n",
    "def extractDF(spark, filepath):\n",
    "    \"\"\"Opens parquet file\n",
    "    \n",
    "    Args:\n",
    "        spark: pyspark.sql.session.SparkSession\n",
    "        filepath: path to downloaded parquet file\n",
    "    Return:\n",
    "        df: list of products, pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    df = spark.read.csv(filepath)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepareModel(df):\n",
    "    \"\"\"Prepare model\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark.sql.dataframe.DataFrame\n",
    "    Return:\n",
    "        model: LinearRegression\n",
    "    \"\"\"\n",
    "    \n",
    "    mapping = {'Iris-virginica' : \"1\", \"Iris-setosa\" : \"2\", \"Iris-versicolor\" : \"3\"}\n",
    "    dfIrisLabeled = df.withColumnRenamed('_c4', 'label')\n",
    "    dfIrisLabeled = dfIrisLabeled.replace(to_replace=mapping, subset=['label'])\n",
    "    dfIrisLabeled = dfIrisLabeled.withColumn(\"label\", dfIrisLabeled.label.cast('int'))\n",
    "    dfIrisInt = (dfIrisLabeled.withColumn(\"_c0\", dfIrisLabeled._c0.cast('int'))\n",
    "                     .withColumn(\"_c1\", dfIrisLabeled._c1.cast('int'))\n",
    "                     .withColumn(\"_c2\", dfIrisLabeled._c2.cast('int'))\n",
    "                     .withColumn(\"_c3\", dfIrisLabeled._c3.cast('int')))\n",
    "    assembler = VectorAssembler(inputCols = ['_c0', '_c1', '_c2', '_c3'], outputCol='features')\n",
    "    output = assembler.transform(dfIrisInt)\n",
    "    finalisedData = output.select('features', 'label')\n",
    "    lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True, labelCol='label', featuresCol='features')\n",
    "    fitModel = lr.fit(finalisedData)\n",
    "    return fitModel\n",
    "\n",
    "\n",
    "def predictModel(spark, model):\n",
    "    \"\"\"Predict given results model\n",
    "    \n",
    "    Args:\n",
    "        spark: pyspark.sql.session.SparkSession\n",
    "        model: LinearRegression\n",
    "    Return:\n",
    "        df: dataframe with predictions, pyspark.sql.dataframe.DataFrame\n",
    "    \"\"\"\n",
    "    predData = spark.createDataFrame(\n",
    "        [(5.1, 3.5, 1.4, 0.2),\n",
    "         (6.2, 3.4, 5.4, 2.3)],\n",
    "        [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
    "    assembler = VectorAssembler(inputCols = ['sepal_length',\n",
    "                                             'sepal_width',\n",
    "                                             'petal_length',\n",
    "                                             'petal_width'],\n",
    "                                outputCol='features')\n",
    "    predDataAcc = assembler.transform(predData)\n",
    "    predFeatures = predDataAcc.select('features')\n",
    "    predicts = model.transform(predFeatures)\n",
    "    predictsPrepared = (predicts.withColumn(\"features\", f.col('features').cast(StringType()))\n",
    "                        .withColumn(\"rawPrediction\", f.col('rawPrediction').cast(StringType()))\n",
    "                        .withColumn(\"probability\", f.col('probability').cast(StringType()))\n",
    "                        .withColumn(\"prediction\", f.col('prediction').cast(StringType())))\n",
    "    return predictsPrepared\n",
    "\n",
    "\n",
    "def loadDF(df, outPutPath):\n",
    "    \"\"\"Load DF into folder\n",
    "    \n",
    "    Args:\n",
    "        df: pyspark.sql.dataframe.DataFrame\n",
    "        outputPath: path to save files\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df.coalesce(1).write.csv(outPutPath)\n",
    "    \n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL script definition\n",
    "    \n",
    "    \"\"\"\n",
    "    filePath = '../files/iris.data'\n",
    "    outPutPath = '../out/out_3_2.txt' \n",
    "    \n",
    "    spark = startSpark()\n",
    "    df = extractDF(spark, filePath)\n",
    "    model = prepareModel(df)\n",
    "    dfPred = predictModel(spark, model)\n",
    "    loadDF(dfPred, outPutPath)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2cb13-a526-4997-8c24-b362d4250c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e073fb9-cd15-48d2-b289-da4ce83b545c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eb1e1-2f81-4b81-8ece-f6e99a11d04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bff3c9-d0dc-4e83-be82-370efd062bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
